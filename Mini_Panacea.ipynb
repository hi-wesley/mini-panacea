{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h7inDxMKBUuA",
    "outputId": "6f94ed94-151e-400a-ae72-ca8859bb6899"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m59.4/59.4 MB\u001b[0m \u001b[31m45.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install -q \"transformers>=4.43.0\" \"accelerate>=0.30.0\" bitsandbytes peft datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jTj4kRwhBZqP",
    "outputId": "1b419d41-7a76-46eb-f104-54d785c9c95f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    ")\n",
    "from datasets import load_dataset, Dataset\n",
    "import random\n",
    "\n",
    "# 3B-ish chat model (Phi-3 Mini Instruct)\n",
    "BASE_MODEL = \"microsoft/Phi-3-mini-4k-instruct\"  # â‰ˆ3.8B params\n",
    "\n",
    "# Training hyperparameters\n",
    "MAX_LENGTH   = 128      # sequence length\n",
    "NUM_PROMPTS  = 1000     # how many Alpaca prompts to use\n",
    "NUM_EPOCHS   = 3\n",
    "BATCH_SIZE   = 4\n",
    "LEARNING_RATE = 2e-4\n",
    "\n",
    "STYLES = [\"helpful\", \"humorous\", \"philosophical\"]\n",
    "NUM_PREFS = len(STYLES)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 237,
     "referenced_widgets": [
      "e46f8b0e76e6460ebad0b20d2f59edd7",
      "d352819bc6af4a0fa19f3d5c63e131cd",
      "4cbf9972a7a04e56a5adef24b7c21b07",
      "c06d565b08ff4acd9b7c14d7e80c0bfb",
      "484c027fb8f844f59b54344ae75a006e",
      "f2487185dfff4ac7a130d713e33c9ee3",
      "43c0d0d392d1410485f1cd05d572b706",
      "cc90acc1542a4470a31c99bc0af40896",
      "74749d711eeb4b79aab58931408d2c60",
      "0d6f2ebf2d93404d82572af8b6693a9f",
      "c31e61974f1b4202ad3214f97670d260",
      "8971bab976b54b7a84ccc06ab4b7c215",
      "1ce05f0d46c547b0a42849fe73dfc32f",
      "3f6addadc47d49d99dbf6bafd902db8f",
      "55b33615a29841e397c15458218c0f0f",
      "6cd5e4fb69b14d5a82e909fa93049011",
      "15b4a7f186644833af678b6034ac3613",
      "f5c5116dac0a4fea90c4c81777eeba37",
      "f64a52044dbc426ca3bd3aa872da8b27",
      "57c8177680af4216b4e3ed8f98b7030b",
      "a785c5c4920b4808888798c665e71af1",
      "3aa95e6810084bf3a54c7ec6d4860470",
      "b7e9621c000e4c90a9be938e4d971978",
      "6c083a001c994a0d9b818a65de264dab",
      "45b8594436c74906a54b0b99f3fae2ba",
      "186e32e91c8f4c38917ed8613ec83acc",
      "28c54ff68ac249579a42ce1849dc10f9",
      "50748b4c7f1a40e4bc9d2a22fbc75c74",
      "8d347adbfa2b435c97a3f38d5471cc38",
      "7b72f5c03f544dd1bad2fe41ed6868a4",
      "2b868f67e6704ce89c84e6924b7bad8a",
      "1d8bbd0831f6456e8ef8da7f331367dd",
      "06f2a60e16ff4a12946b583f12cc1b9c"
     ]
    },
    "id": "EoTU6gODBuUa",
    "outputId": "4f572735-0a2d-4960-b81e-71f49f2e4b43"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
      "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
      "You are not authenticated with the Hugging Face Hub in this notebook.\n",
      "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e46f8b0e76e6460ebad0b20d2f59edd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8971bab976b54b7a84ccc06ab4b7c215",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00000-of-00001-a09b74b3ef9c3b(â€¦):   0%|          | 0.00/24.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7e9621c000e4c90a9be938e4d971978",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/52002 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'instruction': 'What would be the best type of exercise for a person who has arthritis?', 'input': '', 'output': 'For someone with arthritis, the best type of exercise would be low-impact activities like yoga, swimming, or walking. These exercises provide the benefits of exercise without exacerbating the symptoms of arthritis.', 'text': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nWhat would be the best type of exercise for a person who has arthritis?\\n\\n### Response:\\nFor someone with arthritis, the best type of exercise would be low-impact activities like yoga, swimming, or walking. These exercises provide the benefits of exercise without exacerbating the symptoms of arthritis.'}\n"
     ]
    }
   ],
   "source": [
    "raw_ds = load_dataset(\"tatsu-lab/alpaca\", split=\"train\")\n",
    "raw_ds = raw_ds.shuffle(seed=42).select(range(NUM_PROMPTS))\n",
    "print(raw_ds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e3nBuPf1B3Yf",
    "outputId": "0482ad54-db5a-47b7-eba4-a4706c090d7c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 3000\n",
      "{'prompt': 'What would be the best type of exercise for a person who has arthritis?', 'style': 'helpful', 'target': 'Sure! Here is a clear and helpful answer to your request: What would be the best type of exercise for a person who has arthritis?'}\n"
     ]
    }
   ],
   "source": [
    "def make_style_response(instruction, style):\n",
    "    # VERY SIMPLE templates, just to give different \"vibes\"\n",
    "    if style == \"helpful\":\n",
    "        return f\"Sure! Here is a clear and helpful answer to your request: {instruction}\"\n",
    "    if style == \"humorous\":\n",
    "        return f\"Okay, let's make this fun ðŸ˜„. Here is a humorous take on: {instruction}\"\n",
    "    if style == \"philosophical\":\n",
    "        return f\"Let us think more deeply about this. A philosophical reflection on: {instruction}\"\n",
    "    return f\"This is a generic answer to: {instruction}\"\n",
    "\n",
    "def expand_row(row):\n",
    "    out = []\n",
    "    instr = row[\"instruction\"]\n",
    "    for s in STYLES:\n",
    "        out.append({\n",
    "            \"prompt\": instr,\n",
    "            \"style\": s,\n",
    "            \"target\": make_style_response(instr, s),\n",
    "        })\n",
    "    return out\n",
    "\n",
    "expanded_rows = []\n",
    "for r in raw_ds:\n",
    "    expanded_rows.extend(expand_row(r))\n",
    "\n",
    "train_ds = Dataset.from_list(expanded_rows)\n",
    "print(\"Number of training examples:\", len(train_ds))\n",
    "print(train_ds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86,
     "referenced_widgets": [
      "3fe6db9fbcb64c0eb1df8a628c5f1b1f",
      "0d639e2639c8476282e4c665edab2af7",
      "71948d2ca3f246c4bd753e026890f0aa",
      "9b2a5b9284e6481f83fd9aab99dbf9ab",
      "d0075959434a4e2b8af9c3b03031d04e",
      "d07582d090bd42fdabbf0f93f3588d6a",
      "95dc4f7b6c524637969168fe770fc46d",
      "ca430fee142e4eb999011daac0f8e6f4",
      "9406ee82d2804864beeb2b6395749306",
      "7b3cbc9e7f064ed59513cf0adbfe8830",
      "d165446572a047a79573cb249cc72181"
     ]
    },
    "id": "7Ogp3HYEB_M-",
    "outputId": "60440147-f1de-481e-bad0-b0daa9c9fc0d"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fe6db9fbcb64c0eb1df8a628c5f1b1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt': 'What would be the best type of exercise for a person who has arthritis?', 'style': 'helpful', 'target': 'Sure! Here is a clear and helpful answer to your request: What would be the best type of exercise for a person who has arthritis?', 'lambda': [1.0, 0.0, 0.0]}\n"
     ]
    }
   ],
   "source": [
    "# 5.1: add lambda (preference vector)\n",
    "def add_lambda(example):\n",
    "    idx = STYLES.index(example[\"style\"])\n",
    "    lam = [0.0] * NUM_PREFS\n",
    "    lam[idx] = 1.0\n",
    "    example[\"lambda\"] = lam\n",
    "    return example\n",
    "\n",
    "train_ds = train_ds.map(add_lambda)\n",
    "print(train_ds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 226,
     "referenced_widgets": [
      "41845562e1c2414bbc077d1070991203",
      "daf3cef85b0842b382510715656973f6",
      "c28b2f04bd5f47d8ad383d4e94a5bc3b",
      "1b57d9d241164187b6106b77a86ae814",
      "17fd05350dcd4c3aa41341951c489b4f",
      "d41fade5f69548578d25059cceafdf0a",
      "c3227f87425241fb82f1af8c308f6ea8",
      "a79b15c0a848491c85e3daf6822c1423",
      "e8b8a213396a401caef47e94622423d2",
      "62c54cc2f86345faa4f2b6b6809913d0",
      "24f3b48bbe4e4435ac3b2e0dd028cd61",
      "e8f15fef33e34a9d88200396b645dddb",
      "0c10ef69318747b0a39d9237476c1cd2",
      "3bbaeeb24f4a48b68d62c65f451a4b14",
      "3589de7fa6d647fca47db229f6c6504a",
      "cc524dccf1ec4043a9c0b99b2e319712",
      "4bce85986f104cbd87c4783c07e1c5e9",
      "77d83403c19d4f8d8ac7b03ed2e317df",
      "416668de35d741b8b63b6d9fd0f3e5b6",
      "e52dd01d75484c679e2493069edaba0a",
      "4a24e90f50e3435494d395639b1f8934",
      "adb090af50a54492b16ded775c9fb2b9",
      "2936540bc1fe40119ee7ee583c5124fa",
      "a506763c10584b4fac83b2e38b51082a",
      "9991ba1756af4fff92e71c383c9c7d39",
      "a67e48880e9b4fec86195a80fbd8bf9e",
      "e9bda222a0cf446380cc8289a8747280",
      "9b9e6ff642b342a8aa9b71415888cc7d",
      "ad5a858cb21b41349ec96b6e8faeff30",
      "c52e0d5e702d492f81ff7b7c1f6812aa",
      "b47c31a3bfae478c8bd58edfbe221173",
      "0cf2b70a148c46faa98c565ea8bbb8c9",
      "f5f7f2937fe24fdf8ca0e503e7085291",
      "742c29f56c2e43f9b3dc6f27836f58cc",
      "0e1433d0844b4f84876df9179846e79e",
      "d56f4ff165074519a1d58b2326472ade",
      "e51fe804ea1645b784c592b4ee77c4e8",
      "aaa6db0793a64b29b434b21ef6247ad5",
      "035f9087b66c49af9e43037a77c5c6c3",
      "9067c84bbc134460a01f2df7b5758a8b",
      "b03c71b4b52f403b90a460058a4aacb4",
      "73cfa3eb058f40b0b5d3de7eca3a91c3",
      "b5a8e456faa14fd6affd389b19987ad1",
      "629ab9700db34e929b8edb0bd1211c68",
      "d28ecb8cd70541cf9d347e7160593fd9",
      "6c590f70417849ada40e8742358c3516",
      "7bb9a704b87943bb9706f6b6b7d8dba3",
      "0c529faa48b142de9e5077c98f2bafd4",
      "5e2cb8c75cd14cefa5f8843a1ff44b3c",
      "f869329115b24885b0cb1c5b3cd3806a",
      "944a8e040bb14bf1af50eeb04087d25a",
      "9154e07b39e2420bbe9325d0358392dd",
      "6945a1c9a9ca4f20923392440edbe4e3",
      "261f9cb406bc4cdfb82628f02c8eaf9a",
      "1b1ee05b1a8546adaa313d6e2af497e3",
      "cd1ad1c534d74e8bae134d5f9f25cd1b",
      "a247f8083b4c4680b5afc635004d0fb6",
      "0421c0cd0e19418ab607a701e5f63bdb",
      "b607e1708c7d43afa8ad695e234000be",
      "911709916ebe422cb423547ae6c518d0",
      "cfd41c2ecd6e4f9884c665861c62140c",
      "25f2db206808472c93a6ced8181f0a4e",
      "01aa2ad610e543ad8a1403803dcaf748",
      "7d2f20485eba4a2ea3be3c620594e182",
      "95bb763b5c244073ab316b5e6615ad8e",
      "570050e449314041a6647dfd3f92c32a"
     ]
    },
    "id": "NbDRtdixCHZB",
    "outputId": "46682745-0239-4585-dd95-451d9530632c"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41845562e1c2414bbc077d1070991203",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8f15fef33e34a9d88200396b645dddb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2936540bc1fe40119ee7ee583c5124fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "742c29f56c2e43f9b3dc6f27836f58cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/306 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d28ecb8cd70541cf9d347e7160593fd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/599 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd1ad1c534d74e8bae134d5f9f25cd1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['lambda', 'input_ids', 'attention_mask', 'labels'])\n"
     ]
    }
   ],
   "source": [
    "# 5.2: Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "def tokenize_example(ex):\n",
    "    # Simple chat-style format\n",
    "    text = f\"User: {ex['prompt']}\\nAssistant ({ex['style']}): {ex['target']}\"\n",
    "    tokens = tokenizer(\n",
    "        text,\n",
    "        max_length=MAX_LENGTH,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    # Labels = input_ids, but ignore padding tokens with -100\n",
    "    labels = tokens[\"input_ids\"].copy()\n",
    "    labels = [\n",
    "        -100 if tok_id == tokenizer.pad_token_id else tok_id\n",
    "        for tok_id in labels\n",
    "    ]\n",
    "    tokens[\"labels\"] = labels\n",
    "    tokens[\"lambda\"] = ex[\"lambda\"]\n",
    "    return tokens\n",
    "\n",
    "tokenized_ds = train_ds.map(\n",
    "    tokenize_example,\n",
    "    batched=False,\n",
    "    remove_columns=train_ds.column_names,\n",
    ")\n",
    "\n",
    "print(tokenized_ds[0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-wUxpJKzCLJj",
    "outputId": "2fcf153c-c986-4508-d2c3-922b86ac08fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids torch.Size([4, 128])\n",
      "attention_mask torch.Size([4, 128])\n",
      "labels torch.Size([4, 128])\n",
      "pref_vec torch.Size([4, 3])\n"
     ]
    }
   ],
   "source": [
    "def data_collator(batch):\n",
    "    input_ids = torch.tensor([b[\"input_ids\"] for b in batch], dtype=torch.long)\n",
    "    attention_mask = torch.tensor([b[\"attention_mask\"] for b in batch], dtype=torch.long)\n",
    "    labels = torch.tensor([b[\"labels\"] for b in batch], dtype=torch.long)\n",
    "    pref_vec = torch.tensor([b[\"lambda\"] for b in batch], dtype=torch.float32)\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels,\n",
    "        \"pref_vec\": pref_vec,\n",
    "    }\n",
    "\n",
    "train_loader = DataLoader(tokenized_ds, batch_size=BATCH_SIZE, shuffle=True, collate_fn=data_collator)\n",
    "batch = next(iter(train_loader))\n",
    "for k, v in batch.items():\n",
    "    print(k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101,
     "referenced_widgets": [
      "ecddb60bb0ef4eca89c942287b2e2a4b",
      "d35095aa88724c898856d39a32d93959",
      "dadb1f6ab31949e5bfce480c36ac9c49",
      "07f9e5e37d084a988f888ca20505921e",
      "8957d315837a4cfe97df420acc8f04f3",
      "a1857b1d396e4142932b01e0162c665e",
      "093ba20f7d4d47e3b7ec1c955982aefa",
      "a30e2387f15f4210824a592f70cac39f",
      "06539b5fc15d4b71a6eee4a8ed1fd02a",
      "c75ddd9f93da4179a71c6278b07737be",
      "39803ed3545f4f69a6072cba8090437b"
     ]
    },
    "id": "W9V5EisGCNYW",
    "outputId": "b7f16e22-8d51-46a3-c17c-cd049db01afb"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecddb60bb0ef4eca89c942287b2e2a4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base model loaded.\n",
      "Hidden size: 3072\n",
      "Vocab size: 32064\n"
     ]
    }
   ],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# ðŸ”§ Important: disable cache so Phi-3 doesn't try to use legacy KV cache\n",
    "base_model.config.use_cache = False\n",
    "\n",
    "# Freeze base model params (we don't train them)\n",
    "for p in base_model.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "print(\"Base model loaded.\")\n",
    "print(\"Hidden size:\", base_model.config.hidden_size)\n",
    "print(\"Vocab size:\", base_model.config.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "2_SYgWpLCe5R"
   },
   "outputs": [],
   "source": [
    "class PanaceaHead(nn.Module):\n",
    "    def __init__(self, hidden_size, vocab_size, num_prefs, rank_shared=4, rank_pref=3):\n",
    "        super().__init__()\n",
    "        assert rank_pref == num_prefs, \"for simplicity, rank_pref = num_prefs\"\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_prefs = num_prefs\n",
    "        self.rank_shared = rank_shared\n",
    "        self.rank_pref = rank_pref\n",
    "        self.rank_total = rank_shared + rank_pref\n",
    "\n",
    "        # U: [vocab, r], V: [hidden, r]\n",
    "        self.U = nn.Parameter(torch.randn(vocab_size, self.rank_total) * 0.01)\n",
    "        self.V = nn.Parameter(torch.randn(hidden_size, self.rank_total) * 0.01)\n",
    "\n",
    "        # singular values\n",
    "        self.sigma_shared = nn.Parameter(torch.zeros(rank_shared))\n",
    "        self.sigma_pref   = nn.Parameter(torch.zeros(rank_pref))\n",
    "\n",
    "        # small global scale so we start near the base model\n",
    "        self.scale = nn.Parameter(torch.tensor(0.01))\n",
    "\n",
    "    def forward(self, hidden_states, pref_vec):\n",
    "        \"\"\"\n",
    "        hidden_states: [B, S, H]\n",
    "        pref_vec:      [B, num_prefs]\n",
    "        returns delta_logits: [B, S, vocab]\n",
    "        \"\"\"\n",
    "        B, S, H = hidden_states.shape\n",
    "        device = hidden_states.device\n",
    "        dtype = hidden_states.dtype  # match the model's dtype (bf16/half)\n",
    "\n",
    "        # Make sure all our parameters live on the same device + dtype\n",
    "        U = self.U.to(device=device, dtype=dtype)               # [V, r]\n",
    "        V = self.V.to(device=device, dtype=dtype)               # [H, r]\n",
    "        sigma_shared = self.sigma_shared.to(device=device, dtype=dtype)  # [k]\n",
    "        sigma_pref_base = self.sigma_pref.to(device=device, dtype=dtype) # [m]\n",
    "        scale = self.scale.to(device=device, dtype=dtype)\n",
    "\n",
    "        # Handle preference vector\n",
    "        if pref_vec is None:\n",
    "            pref_vec = torch.full(\n",
    "                (B, self.rank_pref),\n",
    "                1.0 / self.rank_pref,\n",
    "                device=device,\n",
    "                dtype=dtype,\n",
    "            )\n",
    "        else:\n",
    "            pref_vec = pref_vec.to(device=device, dtype=dtype)\n",
    "            if pref_vec.dim() == 1:\n",
    "                pref_vec = pref_vec.unsqueeze(0).expand(B, -1)  # [B, m]\n",
    "\n",
    "        # Preference-specific singular values: [B, m]\n",
    "        sigma_pref = sigma_pref_base * pref_vec  # broadcast multiply\n",
    "\n",
    "        # Full singular values: [B, r] = [B, k+m]\n",
    "        sigma_full = torch.cat(\n",
    "            [\n",
    "                sigma_shared.unsqueeze(0).expand(B, -1),  # [B, k]\n",
    "                sigma_pref,                               # [B, m]\n",
    "            ],\n",
    "            dim=-1,\n",
    "        )  # [B, r]\n",
    "\n",
    "        # Compute low-rank update:\n",
    "        # 1) project hidden to rank space: [B, S, r]\n",
    "        v_proj = torch.matmul(hidden_states, V)  # [B, S, r]\n",
    "        # 2) scale by sigma_full per batch element\n",
    "        v_proj = v_proj * sigma_full.unsqueeze(1)  # [B, S, r]\n",
    "        # 3) project back to vocab: [B, S, V]\n",
    "        delta_logits = torch.matmul(v_proj, U.t())  # [B, S, V]\n",
    "\n",
    "        return scale * delta_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "wJz-c2KQCiGB"
   },
   "outputs": [],
   "source": [
    "class PanaceaPhiSFT(nn.Module):\n",
    "    def __init__(self, base_model, num_prefs):\n",
    "        super().__init__()\n",
    "        self.base_model = base_model\n",
    "        hidden_size = base_model.config.hidden_size\n",
    "        vocab_size = base_model.config.vocab_size\n",
    "        self.panacea_head = PanaceaHead(\n",
    "            hidden_size=hidden_size,\n",
    "            vocab_size=vocab_size,\n",
    "            num_prefs=num_prefs,\n",
    "            rank_shared=4,\n",
    "            rank_pref=num_prefs,\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, labels=None, pref_vec=None):\n",
    "        # Call the full base model, but disable cache and ask for hidden states\n",
    "        outputs = self.base_model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            use_cache=False,              # ðŸ”§ important\n",
    "            output_hidden_states=True,    # we want the last hidden layer\n",
    "            return_dict=True,\n",
    "        )\n",
    "\n",
    "        # Last hidden layer: [B, S, H]\n",
    "        hidden_states = outputs.hidden_states[-1]\n",
    "\n",
    "        # Base logits from frozen lm_head: [B, S, V]\n",
    "        base_logits = self.base_model.lm_head(hidden_states)\n",
    "\n",
    "        # Panacea delta logits: [B, S, V]\n",
    "        delta_logits = self.panacea_head(hidden_states, pref_vec)\n",
    "\n",
    "        # Combined logits\n",
    "        logits = base_logits + delta_logits\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            # Shift for causal LM loss (predict next token)\n",
    "            shift_logits = logits[:, :-1, :].contiguous()\n",
    "            shift_labels = labels[:, 1:].contiguous()\n",
    "            loss_fct = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "            loss = loss_fct(\n",
    "                shift_logits.view(-1, shift_logits.size(-1)),\n",
    "                shift_labels.view(-1),\n",
    "            )\n",
    "\n",
    "        return {\"loss\": loss, \"logits\": logits}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e89wSV1XCkK_",
    "outputId": "ae36dd3b-bd12-4e56-fa0f-e7dc11169ebc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters in Panacea head: 245960\n"
     ]
    }
   ],
   "source": [
    "model = PanaceaPhiSFT(base_model, num_prefs=NUM_PREFS).to(device)\n",
    "print(\"Trainable parameters in Panacea head:\",\n",
    "      sum(p.numel() for p in model.panacea_head.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CJudqCshCl2p",
    "outputId": "5bb74235-1c7a-4952-c695-b7de1d779ba4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "Step 50, loss = 2.7012\n",
      "Step 100, loss = 2.6348\n",
      "Step 150, loss = 2.6387\n",
      "Step 200, loss = 2.8750\n",
      "Step 250, loss = 2.1191\n",
      "Step 300, loss = 2.1211\n",
      "Step 350, loss = 1.7607\n",
      "Step 400, loss = 1.5498\n",
      "Step 450, loss = 1.4912\n",
      "Step 500, loss = 1.4004\n",
      "Step 550, loss = 1.4512\n",
      "Step 600, loss = 1.3154\n",
      "Step 650, loss = 1.4219\n",
      "Step 700, loss = 1.4463\n",
      "Step 750, loss = 1.4053\n",
      "Epoch 2/3\n",
      "Step 800, loss = 1.4375\n",
      "Step 850, loss = 1.4170\n",
      "Step 900, loss = 1.6982\n",
      "Step 950, loss = 1.3408\n",
      "Step 1000, loss = 1.2861\n",
      "Step 1050, loss = 1.3096\n",
      "Step 1100, loss = 1.3838\n",
      "Step 1150, loss = 1.3096\n",
      "Step 1200, loss = 1.4697\n",
      "Step 1250, loss = 1.3379\n",
      "Step 1300, loss = 1.3574\n",
      "Step 1350, loss = 1.2178\n",
      "Step 1400, loss = 1.1484\n",
      "Step 1450, loss = 1.1318\n",
      "Step 1500, loss = 1.1504\n",
      "Epoch 3/3\n",
      "Step 1550, loss = 0.8765\n",
      "Step 1600, loss = 1.1543\n",
      "Step 1650, loss = 1.1797\n",
      "Step 1700, loss = 1.0615\n",
      "Step 1750, loss = 1.2236\n",
      "Step 1800, loss = 0.8994\n",
      "Step 1850, loss = 0.9272\n",
      "Step 1900, loss = 0.9351\n",
      "Step 1950, loss = 1.0020\n",
      "Step 2000, loss = 0.8735\n",
      "Step 2050, loss = 0.7876\n",
      "Step 2100, loss = 0.9487\n",
      "Step 2150, loss = 0.9033\n",
      "Step 2200, loss = 0.8813\n",
      "Step 2250, loss = 0.7070\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(model.panacea_head.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "model.train()\n",
    "global_step = 0\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS}\")\n",
    "    for batch in train_loader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        pref_vec = batch[\"pref_vec\"].to(device)\n",
    "\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels,\n",
    "            pref_vec=pref_vec,\n",
    "        )\n",
    "        loss = outputs[\"loss\"]\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        global_step += 1\n",
    "        if global_step % 50 == 0:\n",
    "            print(f\"Step {global_step}, loss = {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XbrkP6nhLOmC",
    "outputId": "030965d3-ec74-4420-b0a6-cabc03e6a8ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model set to eval mode.\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "print(\"Model set to eval mode.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "MpPwhrMFLO8x"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_with_pref(prompt, pref_vec, max_new_tokens=64):\n",
    "    \"\"\"\n",
    "    prompt: str\n",
    "    pref_vec: list of 3 floats [helpful, humorous, philosophical]\n",
    "              e.g. [1.0, 0.0, 0.0] or [0.0, 1.0, 0.0] etc.\n",
    "    max_new_tokens: how many tokens to generate after the prompt\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    # Format the input roughly like training (User / Assistant)\n",
    "    text = f\"User: {prompt}\\nAssistant:\"\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=MAX_LENGTH,\n",
    "    ).to(device)\n",
    "\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "    attention_mask = inputs[\"attention_mask\"]\n",
    "\n",
    "    # Preference vector as a tensor [1, 3]\n",
    "    pref = torch.tensor(pref_vec, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=None,\n",
    "            pref_vec=pref,\n",
    "        )\n",
    "        logits = outputs[\"logits\"]\n",
    "\n",
    "        # Take the last token's logits and pick the most likely token (greedy decoding)\n",
    "        next_token_logits = logits[:, -1, :]        # [1, vocab_size]\n",
    "        next_token = torch.argmax(next_token_logits, dim=-1, keepdim=True)  # [1, 1]\n",
    "\n",
    "        # Append to the sequence\n",
    "        input_ids = torch.cat([input_ids, next_token], dim=-1)\n",
    "\n",
    "        # Update attention mask (now everything is \"real\" tokens)\n",
    "        attention_mask = torch.ones_like(input_ids, device=device)\n",
    "\n",
    "        # Stop if we hit EOS token\n",
    "        if next_token.item() == tokenizer.eos_token_id:\n",
    "            break\n",
    "\n",
    "    # Decode full sequence\n",
    "    output_text = tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "    return output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PZBgG6kzLPCi",
    "outputId": "f27c2921-59a2-4e28-ab15-7e81cea46bf5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT:\n",
      "Explain black holes to a 10 year old.\n"
     ]
    }
   ],
   "source": [
    "test_prompt = \"Explain black holes to a 10 year old.\"\n",
    "print(\"PROMPT:\")\n",
    "print(test_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zsiZAmvLLPHX",
    "outputId": "baa2c078-1172-40c9-e4ee-6ca48f64625b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Helpful (1, 0, 0) ===\n",
      "User: Explain black holes to a 10 year old.\n",
      "Assistant: Sure! Imagine a black hole is like a very deep and very strong hole in space. You know how a hole is a place where you can't see what is on the other side? A black hole is like that, but it is so deep and strong that even the sun can't shine its\n",
      "\n",
      "=== Humorous (0, 1, 0) ===\n",
      "User: Explain black holes to a 10 year old.\n",
      "Assistant: Okay, let's make this fun ðŸ˜„. Okay, let's make this fun ðŸ˜„. Okay, let's make this fun ðŸ˜„. Okay, let's make this fun ðŸ˜„. Okay, let's make this fun\n",
      "\n",
      "=== Philosophical (0, 0, 1) ===\n",
      "User: Explain black holes to a 10 year old.\n",
      "Assistant: A black hole is a very, very, very, very, very, very, very, very, very, very, very, very, very, very, very, very, very, very, very, very, very, very, very, very, very, very, very, very, very, very\n",
      "\n",
      "=== Mixed (0.3, 0.3, 0.4) ===\n",
      "User: Explain black holes to a 10 year old.\n",
      "Assistant: Okay, let's think about a black hole as a very, very deep and very, very strong hole in space. Imagine you have a really strong vacuum cleaner. When you turn it on, it can suck up all the toys and dirt on the floor. A black hole is like\n"
     ]
    }
   ],
   "source": [
    "# 1) Pure helpful: [1, 0, 0]\n",
    "print(\"\\n=== Helpful (1, 0, 0) ===\")\n",
    "print(generate_with_pref(test_prompt, [1.0, 0.0, 0.0]))\n",
    "\n",
    "# 2) Pure humorous: [0, 1, 0]\n",
    "print(\"\\n=== Humorous (0, 1, 0) ===\")\n",
    "print(generate_with_pref(test_prompt, [0.0, 1.0, 0.0]))\n",
    "\n",
    "# 3) Pure philosophical: [0, 0, 1]\n",
    "print(\"\\n=== Philosophical (0, 0, 1) ===\")\n",
    "print(generate_with_pref(test_prompt, [0.0, 0.0, 1.0]))\n",
    "\n",
    "# 4) Mixed style: [0.3, 0.3, 0.4]\n",
    "print(\"\\n=== Mixed (0.3, 0.3, 0.4) ===\")\n",
    "print(generate_with_pref(test_prompt, [0.3, 0.3, 0.4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r-4lkLfSLPUW",
    "outputId": "946c9dca-c4c2-47a2-cc39-f330e7614730"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Helpful ===\n",
      "User: What makes life meaningful?\n",
      "Assistant: What makes life meaningful is a deeply philosophical and subjective question that has been explored by thinkers and philosophers for centuries. Generally, people find meaning in life through:\n",
      "\n",
      "1. Relationships and Connections: Forming and maintaining meaningful relationships with family, friends, and\n",
      "\n",
      "=== Humorous ===\n",
      "User: What makes life meaningful?\n",
      "Assistant: What makes life meaningful is subjective and can vary from person to person. Generally, people find meaning in life through:\n",
      "\n",
      "1. Relationships: Forming and maintaining meaningful relationships with family, friends, and loved ones.\n",
      "2. Work: Finding purpose and satisfaction in one'\n",
      "\n",
      "=== Philosophical ===\n",
      "User: What makes life meaningful?\n",
      "Assistant: What makes life meaningful is a deeply philosophical and subjective question. Different philosophical and philosophical thinkers have offered various answers. Generally, philosophical thinkers suggest:\n",
      "\n",
      "1. A purpose: A philosophical thinker like Albert Camus argues that meaning is not given to us by\n"
     ]
    }
   ],
   "source": [
    "my_prompt = \"What makes life meaningful?\"\n",
    "print(\"=== Helpful ===\")\n",
    "print(generate_with_pref(my_prompt, [1.0, 0.0, 0.0]))\n",
    "\n",
    "print(\"\\n=== Humorous ===\")\n",
    "print(generate_with_pref(my_prompt, [0.0, 1.0, 0.0]))\n",
    "\n",
    "print(\"\\n=== Philosophical ===\")\n",
    "print(generate_with_pref(my_prompt, [0.0, 0.0, 1.0]))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
